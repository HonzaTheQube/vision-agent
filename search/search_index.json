{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quick start","text":""},{"location":"#welcome-to-the-landing-ai-lmm-tools-documentation","title":"Welcome to the Landing AI LMM Tools Documentation","text":"<p>This library provides a set of tools to help you build applications with Large Multimodal Model (LMM).</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#install","title":"Install","text":"<p>First, install the library:</p> <pre><code>pip install vision-agent\n</code></pre>"},{"location":"#lmms","title":"LMMs","text":"<p>One of the problems of dealing with image data is it can be difficult to organize and search. For example, you might have a bunch of pictures of houses and want to count how many yellow houses you have, or how many houses with adobe roofs. The vision agent library uses LMMs to help create tags or descriptions of images to allow you to search over them, or use them in a database to carry out other operations.</p> <p>To get started, you can use an LMM to start generating text from images. The following code will use the LLaVA-1.6 34B model to generate a description of the image you pass it.</p> <pre><code>import vision_agent as va\n\nmodel = va.lmm.get_lmm(\"llava\")\nmodel.generate(\"Describe this image\", \"image.png\")\n&gt;&gt;&gt; \"A yellow house with a green lawn.\"\n</code></pre> <p>WARNING We are hosting the LLaVA-1.6 34B model, if it times out please wait ~3-5 min for the server to warm up as it shuts down when usage is low.</p>"},{"location":"#datastore","title":"DataStore","text":"<p>You can use the <code>DataStore</code> class to store your images, add new metadata to them such as descriptions, and search over different columns.</p> <pre><code>import vision_agent as va\nimport pandas as pd\n\ndf = pd.DataFrame({\"image_paths\": [\"image1.png\", \"image2.png\", \"image3.png\"]})\nds = va.data.DataStore(df)\nds = ds.add_lmm(va.lmm.get_lmm(\"llava\"))\nds = ds.add_embedder(va.emb.get_embedder(\"sentence-transformer\"))\n\nds = ds.add_column(\"descriptions\", \"Describe this image.\")\n</code></pre> <p>This will use the prompt you passed, \"Describe this image.\", and the LMM to create a new column of descriptions for your image. Your data will now contain a new column with the descriptions of each image:</p> image_paths image_id descriptions image1.png 1 \"A yellow house with a green lawn.\" image2.png 2 \"A white house with a two door garage.\" image3.png 3 \"A wooden house in the middle of the forest.\" <p>You can now create an index on the descriptions column and search over it to find images that match your query.</p> <pre><code>ds = ds.build_index(\"descriptions\")\nds.search(\"A yellow house.\", top_k=1)\n&gt;&gt;&gt; [{'image_paths': 'image1.png', 'image_id': 1, 'descriptions': 'A yellow house with a green lawn.'}]\n</code></pre> <p>You can also create other columns for you data such as <code>is_yellow</code>:</p> <pre><code>ds = ds.add_column(\"is_yellow\", \"Is the house in this image yellow? Please answer yes or no.\")\n</code></pre> <p>which would give you a dataset similar to this:</p> image_paths image_id descriptions is_yellow image1.png 1 \"A yellow house with a green lawn.\" \"yes\" image2.png 2 \"A white house with a two door garage.\" \"no\" image3.png 3 \"A wooden house in the middle of the forest.\" \"no\""}]}